The WinnerGrad Competition – Testing for Intelligence

In a world where AI continually pushes boundaries, the WinnerGrad competition introduces an innovative dimension by challenging AI systems not only to solve problems but also to predict their own accuracy. This evolution promises to shed light on the fascinating realm of self-awareness in AI, offering a new perspective on the capabilities and limitations of these advanced systems.

This was a group submission as part of an assignment for the MS in Artificial Intelligence at the University of Bath

Feedback from our submission:
Our group recieved a distinction for our paper, this was the feedback from our supervisor:

Intelligence (7.5/10): Intelligence is described but could be better defined, for example, by identifying a specific definition from an academic resource. This definition could also support the claim that “there is a widespread acknowledgement of genetics playing a significant role”. The different aspects are well-presented although could be backed by more sources, for example, to support the claim that “Context-sensitivity is a crucial aspect of human intelligence”. 


Conditions and process (8.5/10): An interesting test based on the WinoGrande challenge. The prediction is a good addition. The example is very useful to understand how problems are presented. The image shows how the tasks are scored but they only indicate the maximum score. More details would have enhanced the description, also through an image/diagram. For example: Can participants get fewer points? How does the challenge specifically work? For example, is this just amongst AI or are humans and computers competing? If so, how are the final scores of each group calculated and compared?


Feasibility and challenges (8/10): The idea of adding self-reflection is what makes this test particularly interesting. Your description of the challenges is well done. Indeed, the introduction of self-reflection may increase the difficulty of the test. However, this statement would have been stronger if supported by a citation (e.g. an academic publication, a report, a news article). A citation may also help justify your prediction of scores falling below 70% as well as support other claims such as “it is not uncommon for AI systems to be sure they are correct until explicitly corrected”.
